# 解决 0-100 加减法数学题的三个深度学习模型

本项目展示了三种不同的深度学习模型，用于解决基本的加减法数学题。所有模型均使用 PyTorch 实现，并通过自动生成的数学数据进行训练。每个模型源文件中均包含详细注释，帮助初学者理解数据处理、模型设计、训练和推断的全过程。

---

## Overview

本项目包含以下三个模型：

1. **MoE Transformer 模型** (`src/moe_transformer.py`)
   - **概述**：  
     本模型采用 Mixture of Experts (MoE) 架构，通过一个门控网络计算各专家（Transformer Encoder 层）的权重，选择前 2 个最重要的专家对输入进行处理，然后加权融合所有专家的输出，最终预测数学题的答案。
   - **关键点**：
     - 学习嵌入层与位置编码的作用。
     - 理解门控网络如何根据输入动态调节专家的贡献。
     - 掌握多专家输出如何融合提高预测效果。

2. **Standard Transformer 模型** (`src/transformer_math_solver.py`)
   - **概述**：  
     该模型采用标准的 Transformer Encoder 架构，将输入通过嵌入和位置编码后送入 Transformer 编码器，经过全局特征提取后，通过平均池化和全连接层输出预测结果。
   - **关键点**：
     - 熟悉 Transformer 的基本结构和工作流程。
     - 理解序列平均池化在生成固定长度表示中的作用。

3. **LSTM 算术求解器** (`src/lstm_math_solver.py`)
   - **概述**：  
     作为基线模型，本方法采用 LSTM 处理序列。输入经过嵌入层转为向量，随后由 LSTM 层提取时序特征，最后利用全连接层输出数值预测。
   - **关键点**：
     - 理解循环神经网络（特别是 LSTM）的基本结构与原理。
     - 掌握如何将时间序列信息转化为固定长度的向量表示。

---

## 环境及依赖

### 前置条件
- **Python 3.x**（推荐 3.8 及以上版本）
- **PyTorch** ≥ 1.7.0
- **NumPy** ≥ 1.21.0
- **Matplotlib** ≥ 3.4.0（可选，用于数据和结果的可视化）

### 安装依赖
请使用以下命令安装 `requirements.txt` 中列出的所有依赖：
```bash
pip install -r requirements.txt
```

---

## 项目结构

```
.
├── src
│   ├── moe_transformer.py         # MoE Transformer 模型实现及详细注释
│   ├── transformer_math_solver.py # Standard Transformer 模型实现及详细注释
│   └── lstm_math_solver.py        # LSTM 算术求解器实现及详细注释
├── requirements.txt               # 项目依赖清单
└── readme.md                      # 详细的学生版使用说明
```

---

## 详细说明

### 数据生成与预处理

- **数据生成**  
  每个模型均通过相应的函数生成加减法运算数据（例如 `"25 + 30"`），并计算出答案（以字符串形式）。  
  *教学要点*：如何模拟生成训练数据，将现实问题转化为模型可处理的格式。

- **词汇表构建**  
  遍历问题与答案中的所有字符，构建字符到索引的映射，同时预留 `<PAD>`（用于填充）和 `<UNK>`（用于未知字符）。  
  *教学要点*：文本预处理的重要性及其实现方法。

- **预处理**  
  将输入问句转换为数值序列，并统一序列长度（通过填充）以便批处理。  

### 模型架构详解

#### 1. MoE Transformer 模型 (`src/moe_transformer.py`)
- **嵌入层与位置编码**  
  将输入字符索引转换为向量表示，并加入固定的位置信息，使模型捕捉序列顺序。
  
- **门控网络**  
  基于句子整体信息（通过平均池化获得）计算每个专家的得分，输出经过 softmax 转换后的概率分布，作为各专家的权重。
  
- **专家网络**  
  多个 Transformer Encoder 层分别独立处理输入，输出经过平均池化后得到固定长度向量表示。  
  *教学要点*：如何构造多专家模型，并利用不同专家提取不同特征。

- **输出层**  
  将选中的专家输出按权重加权融合，再通过全连接层输出最终预测结果。

#### 2. Standard Transformer 模型 (`src/transformer_math_solver.py`)
- **基本流程**  
  从嵌入和位置编码开始，将数据传入 Transformer 编码器，经过全局特征提取后对序列进行平均池化，最后由全连接层生产预测输出。  
  *教学要点*：了解 Transformer 编码器如何学习序列全局表示。

#### 3. LSTM 算术求解器 (`src/lstm_math_solver.py`)
- **基本流程**  
  通过嵌入层转化输入序列，随后由 LSTM 层提取时序特征，并利用最后一步隐藏状态通过全连接层输出数值预测。  
  *教学要点*：探究 LSTM 在序列数据处理方面的优势与局限。

### 训练与评估

- **训练循环**  
  对所有模型均采用均方误差（MSELoss）作为损失函数，使用 Adam 优化器更新模型权重。数据将按照批次（或逐样本）输入，训练过程中打印每个 Epoch 的平均损失以监控训练进度。  

- **评估与预测**  
  在预测时，将模型切换为评估模式（关闭 Dropout 等训练特定操作），将输入进行编码、填充后传入模型，最后输出经过四舍五入处理的预测结果。

---

## 如何运行示例

在终端执行以下命令启动对应模型的训练和测试：

- **运行 MoE Transformer 模型**：
  ```bash
  python src/moe_transformer.py
  ```

- **运行 Standard Transformer 模型**：
  ```bash
  python src/transformer_math_solver.py
  ```

- **运行 LSTM 算术求解器**：
  ```bash
  python src/lstm_math_solver.py
  ```

运行后，在终端可查看训练过程中每个 Epoch 的损失变化以及部分测试问题的预测结果。

---

## Future Improvements

- **模型优化**：  
  调整模型结构和超参数以进一步提升预测精度。
  
- **数据扩展**：  
  引入更复杂的算术问题（如乘除法）以提升模型的泛化能力。

- **功能增强**：  
  添加模型保存与加载机制，以及更多评估指标对比实验。

---

## 总结

本项目不仅展示了如何解决简单数学问题的多种深度学习方法，还为初学者提供了一个详细的案例，涵盖数据生成、预处理、模型构建、训练与评估全过程。通过阅读和学习本指南，你将能更深入地理解不同神经网络架构的原理，并通过修改和扩展该代码提高自己的编程和机器学习能力。

Happy coding and learning! 